{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# !pip install tensorboard\n",
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "import copy\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "border = 0.02\n",
    "print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>judgement</th>\n",
       "      <th>title_and_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "      <td>Longitudinal studies indicate that declines in...</td>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "      <td>The present study was undertaken to validate t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "      <td>Objective: To report a case series in which ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "      <td>The etiology and pathogenesis of idiopathic ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>0</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  One-year age changes in MRI brain volumes in o...   \n",
       "1   1  Supportive CSF biomarker evidence to enhance t...   \n",
       "2   2  Occurrence of basal ganglia germ cell tumors w...   \n",
       "3   3  New developments in diagnosis and therapy of C...   \n",
       "4   4  Prolonged shedding of SARS-CoV-2 in an elderly...   \n",
       "\n",
       "                                            abstract  judgement  \\\n",
       "0  Longitudinal studies indicate that declines in...          0   \n",
       "1  The present study was undertaken to validate t...          0   \n",
       "2  Objective: To report a case series in which ba...          0   \n",
       "3  The etiology and pathogenesis of idiopathic ch...          0   \n",
       "4  Prolonged shedding of SARS-CoV-2 in an elderly...          0   \n",
       "\n",
       "                                  title_and_abstract  \n",
       "0  One-year age changes in MRI brain volumes in o...  \n",
       "1  Supportive CSF biomarker evidence to enhance t...  \n",
       "2  Occurrence of basal ganglia germ cell tumors w...  \n",
       "3  New developments in diagnosis and therapy of C...  \n",
       "4  Prolonged shedding of SARS-CoV-2 in an elderly...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_texts(df):\n",
    "    titles_and_abstracts = df['title_and_abstract'].values.tolist()\n",
    "    return titles_and_abstracts\n",
    "\n",
    "def get_labels(df):\n",
    "    labels = df.iloc[:, 3].values\n",
    "    return labels\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    if type(text) == float:\n",
    "        text = ''\n",
    "    #print(text)\n",
    "    \n",
    "    #text = text.lower()\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [x.strip() for x in text]\n",
    "    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n",
    "    text = ' '.join(text)\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "    #text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    \n",
    "    \n",
    "    text = preprocess(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    #stop = stopwords.words('english')\n",
    "    #text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = replace_double_quotation(sentence)\n",
    "    sentence = replace_garbled_text(sentence)\n",
    "    return sentence\n",
    "\n",
    "def replace_garbled_text(sentence):\n",
    "    garbled_char_table = {\n",
    "        'Â©': '©', '–': '-', '‐': '-',\n",
    "        'ﾂ｣': '£', 'ﾂｩ': '©', 'ﾂｫ': '«', 'ﾂｮ': '®', 'ﾂｰ': '°', 'ﾂｱ': '±', 'ﾂｲ': '²', 'ﾂｳ': '³', 'ﾂｴ': '´', 'ﾂｵ': 'µ', 'ﾂｷ': '·', 'ﾂｸ': '¸', 'ﾂｹ': '¹', 'ﾂｼ': '¼', 'ﾂｽ': '½', 'ﾂｾ': '¾', 'ﾂｿ': '¿', 'ﾂ': '',\n",
    "        'ﾃｷ': '÷', 'ﾃｸ': 'ø', 'ﾃ': 'a', 'ﾃ｡': 'a', 'ﾃ｢': 'a', 'ﾃ｣': 'a', 'ﾃ､': 'a', 'ﾃ･': 'a', 'ﾃｦ': 'ae', 'ﾃｧ': 'c', 'ﾃｨ': 'e', 'ﾃｩ': 'e', 'ﾃｪ': 'e', 'ﾃｫ': 'e',\n",
    "        'ﾃｬ': 'i', 'ﾃｭ': 'i', 'ﾃｮ': 'i', 'ﾃｯ': 'i', 'ﾃｱ': 'n', 'ﾃｲ': 'o', 'ﾃｳ': 'o', 'ﾃｴ': 'o', 'ﾃｵ': 'o', 'ﾃｶ': 'o', 'ﾃｹ': 'u', 'ﾃｺ': 'u', 'ﾃｻ': 'u', 'ﾃｼ': 'u', 'ﾃｽ': 'y', 'ﾃｿ': 'y', 'ﾃ': '×', \n",
    "        'ﾎｱ': 'α', 'ﾎｲ': 'β', 'ﾎｳ': 'γ', 'ﾎｴ': 'δ', 'ﾎｵ': 'ε', 'ﾎｶ': 'ζ', 'ﾎｷ': 'η', 'ﾎｸ': 'θ', 'ﾎｹ': 'ι', 'ﾎｺ': 'κ', 'ﾎｻ': 'λ', 'ﾎｼ': 'μ', 'ﾎｽ': 'ν', 'ﾎｾ': 'ξ', 'ﾎｿ': 'ο', 'ﾎ': '',\n",
    "        'ﾏ': ' ',\n",
    "        '竕､': '≤', '竕･': '≥', '竕ｦ': '≦', '竕ｧ': '≧',\n",
    "        '窶｢': '•', '窶ｦ': '…', '窶ｲ': '′', '窶ｳ': '″', '窶ｴ': '‴', '窶': ' ',\n",
    "        '竅ｰ': '⁰', '竅ｴ': '⁴', '竅ｵ': '⁵', '竅ｶ': '⁶', '竅ｷ': '⁷', '竅ｸ': '⁸', '竅ｹ': '⁹', '竅ｺ': '⁺', '竅ｻ': '⁻', '竅ｼ': '⁼', \n",
    "        '竏･': '∥', '竏ｪ': '∪', '竏ｫ': '∫', '竏ｶ': '∶', '竏ｼ': '∼', '竏': '', \n",
    "        'ﾂ\\uf8f0': ' '\n",
    "    }\n",
    "    for garbled_char, valid_char in garbled_char_table.items():\n",
    "        sentence = sentence.replace(garbled_char, valid_char)\n",
    "    sentence = re.sub('[ぁ-んァ-ンｦ-ﾟ一-龥]', '', sentence)\n",
    "    return sentence.translate(garbled_char_table)\n",
    "\n",
    "def replace_double_quotation(sentence):\n",
    "    return sentence.replace('\"', \"'\")  \n",
    "\n",
    "def clean_data(train_df):\n",
    "    train_df.loc[train_df['abstract'].isnull(), 'abstract'] = train_df['title']\n",
    "    train_df['abstract'] = train_df['abstract'].apply(clean_text)\n",
    "    train_df['title'] = train_df['title'].apply(clean_text)\n",
    "    return train_df\n",
    "\n",
    "def concat(train_df):\n",
    "    train_df['title_and_abstract'] = train_df['title'] + train_df['abstract']\n",
    "    return train_df\n",
    "\n",
    "train_df.loc[[2488,7708],['judgement']] = 0\n",
    "train_df = clean_data(train_df)\n",
    "test_df= clean_data(test_df)\n",
    "train_df = concat(train_df)\n",
    "test_df = concat(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        super(Config, self).__init__()\n",
    "        \n",
    "        self.SEED = 42\n",
    "        self.MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n",
    "        self.NUM_LABELS = 1\n",
    "        \n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
    "        self.MAX_LENGTH = 512\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.N_SPLIT = 5\n",
    "        \n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.FULL_FINETUNING = True\n",
    "        self.LR = 2e-5\n",
    "        self.OPTIMIZER = 'AdamW'\n",
    "        self.CRITERION = 'BCEWithLogitsLoss'\n",
    "        self.SAVE_BEST_ONLY = True\n",
    "        self.N_VALIDATE_DUR_TRAIN = 1\n",
    "        self.EPOCHS = 2 \n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_init(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = config.SEED\n",
    "seed_init(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, df, indices, set_type=None):\n",
    "        super(TransformerDataset, self).__init__()\n",
    "\n",
    "        df = df.iloc[indices]\n",
    "        self.titles_and_abstracts = get_texts(df)\n",
    "        self.set_type = set_type\n",
    "        if self.set_type != 'test':\n",
    "            self.labels = get_labels(df)\n",
    "\n",
    "        self.max_length = config.MAX_LENGTH\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles_and_abstracts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokenized_titles_and_abstracts = self.tokenizer.encode_plus(\n",
    "            self.titles_and_abstracts[index], \n",
    "            max_length=self.max_length,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids_titles_and_abstracts = tokenized_titles_and_abstracts['input_ids'].squeeze()\n",
    "        attention_mask_titles_and_abstracts = tokenized_titles_and_abstracts['attention_mask'].squeeze()\n",
    "        \n",
    "\n",
    "        if self.set_type != 'test':\n",
    "            return {\n",
    "                'titles_and_abstracts': {\n",
    "                    'input_ids': input_ids_titles_and_abstracts.long(),\n",
    "                    'attention_mask': attention_mask_titles_and_abstracts.long(),\n",
    "                },\n",
    "                'labels': torch.Tensor([self.labels[index]]).float(),\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'titles_and_abstracts': {\n",
    "                'input_ids': input_ids_titles_abstracts.long(),\n",
    "                'attention_mask': attention_mask_titles_and_abstracts.long(),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubMedBert(nn.Module):\n",
    "    def __init__(self,path):\n",
    "        super(PubMedBert, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(path)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.avgpool = nn.AvgPool1d(2, 2)\n",
    "        self.linear = nn.Linear(768, config.NUM_LABELS)\n",
    "    \n",
    "    def forward(self, input_ids_titles_and_abstracts, attention_mask_titles_and_abstracts=None):\n",
    "        output = self.model(input_ids=input_ids_titles_and_abstracts, attention_mask=attention_mask_titles_and_abstracts)\n",
    "        features = output.pooler_output\n",
    "        features = features.unsqueeze(1)\n",
    "        features_pooled = self.avgpool(features)\n",
    "        features_pooled = features_pooled.squeeze(1)\n",
    "\n",
    "        x = self.dropout(features)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model,model1,model2, val_dataloader, criterion, epoch, model_number):\n",
    "    val_loss = 0\n",
    "    true, pred,  pred1,  pred2, output = [], [], [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        b_input_ids = batch['titles_and_abstracts']['input_ids'].to(device)\n",
    "        b_attention_mask = batch['titles_and_abstracts']['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attention_mask)\n",
    "            logits1=model1(b_input_ids,b_attention_mask)\n",
    "            logits2=model2(b_input_ids,b_attention_mask)\n",
    "            \n",
    "            logits = logits.view(-1, 1)\n",
    "            logits1=logits.view(-1,1)\n",
    "            logits2=logits2.view(-1,1)\n",
    "            \n",
    "            loss = criterion(logits, b_labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            logits = torch.sigmoid(logits)\n",
    "            logits1= torch.sigmoid(logits1)\n",
    "            logits2 = torch.sigmoid(logits2)\n",
    "            \n",
    "            logits = logits.to('cpu').detach().numpy().copy()\n",
    "            logits1 = logits1.to('cpu').detach().numpy().copy()\n",
    "            logits2 = logits2.to('cpu').detach().numpy().copy()\n",
    "            \n",
    "            output.extend(logits.tolist())\n",
    "            logits = np.where(logits < border, 0, 1)\n",
    "            logits1 = np.where(logits1 < border, 0, 1)\n",
    "            logits2 = np.where(logits2 < border, 0, 1)\n",
    "            \n",
    "            labels = b_labels.to('cpu').detach().numpy().copy()\n",
    "            \n",
    "            pred.extend(logits)\n",
    "            pred1.extend(logits1)\n",
    "            pred2.extends(logit2)\n",
    "            print(preds)\n",
    "            print(preds.shape())\n",
    "            true.extend(labels)\n",
    "            \n",
    "#     output0, output1 = [], []\n",
    "#     for p, o in zip(true, output):\n",
    "#         if p == 0:\n",
    "#             output0.append(o[0])\n",
    "#         else:\n",
    "#             output1.append(o[0])\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax1 = fig.add_subplot(1,2,1)\n",
    "#     ax1.hist(output0, bins=100, color='red', alpha=0.5)\n",
    "#     ax1.set_xlabel('output')\n",
    "#     ax1.set_ylabel('sum')\n",
    "#     ax1.set_title('val_0')\n",
    "    \n",
    "#     ax2 = fig.add_subplot(1,2,2)\n",
    "#     ax2.hist(output1, bins=100, color='blue', alpha=0.5)\n",
    "#     ax2.set_xlabel('output')\n",
    "#     ax2.set_ylabel('sum')\n",
    "#     ax2.set_title('val_1')\n",
    "    \n",
    "#     fig.savefig('graph/val_scibert'+str(epoch)+'.png')\n",
    "        \n",
    "#     avg_val_loss = val_loss / len(val_dataloader)\n",
    "#     print('Val loss:', avg_val_loss)\n",
    "#     print('Val accuracy:', accuracy_score(true, pred))\n",
    "    \n",
    "    val_fbeta_score = fbeta_score(true, pred, beta=7.0)\n",
    "    print('Val fbeta score:', val_fbeta_score)\n",
    "    return val_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run( val_dataloader, writer):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = PubMedBert(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "    model.load_state_dict(torch.load(\"biobert_input_best_model.pt\"))\n",
    "    model=nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    model1=PubMedBert(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model1.load_state_dict(torch.load(\"pubmedbert_input_best_model_biomed.pt\"))\n",
    "    model1=nn.DataParallel(model1)\n",
    "    model1.to(device)\n",
    "    \n",
    "    model2=PubMedBert(path=\"bert-base-uncased\")\n",
    "    model2.load_state_dict(torch.load(\"bert.pt\"))\n",
    "    model2=nn.DataParallel(model2)\n",
    "    model2.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "#     if config.FULL_FINETUNING:\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_parameters = [\n",
    "#             {\n",
    "#                 \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#                 \"weight_decay\": 0.001,\n",
    "#             },\n",
    "#             {\n",
    "#                 \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#                 \"weight_decay\": 0.0,\n",
    "#             },\n",
    "#         ]\n",
    "#         optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
    "    \n",
    "#     num_training_steps = len(train_dataloader) * config.EPOCHS\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    max_val_fbeta_score = float('-inf')\n",
    "    for epoch in range(config.EPOCHS):\n",
    "#         avg_train_loss, accuracy = train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n",
    "        max_val_fbeta_score = val(model,model1,model2, val_dataloader, criterion, epoch)\n",
    "        \n",
    "        best_model=copy.deepcopy(model)\n",
    "#         writer.add_scalar('train_loss', avg_train_loss, epoch+1)\n",
    "#         writer.add_scalar('accuracy', accuracy, epoch+1)\n",
    "        writer.add_scalar('val_fbeta_score', val_fbeta_score, epoch+1)\n",
    "        \n",
    "#         if config.SAVE_BEST_ONLY:\n",
    "#             if val_fbeta_score > max_val_fbeta_score:\n",
    "#                 max_val_fbeta_score = val_fbeta_score\n",
    "#                 best_model = copy.deepcopy(model)\n",
    "                \n",
    "    return best_model, max_val_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val():\n",
    "    Fold = StratifiedKFold(n_splits=config.N_SPLIT, shuffle=True, random_state=seed)\n",
    "    max_val_fbeta_score = float('-inf')\n",
    "    \n",
    "    for n, (train_indices, val_indices) in enumerate(Fold.split(train_df, train_df['judgement'])):\n",
    "        print(f'========= fold: {n} training =========')\n",
    "        \n",
    "        log_dir = 'logs/fold_scibert'+str(n)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        train_data = TransformerDataset(train_df, train_indices)\n",
    "        val_data = TransformerDataset(train_df, val_indices)\n",
    "        \n",
    "#         train_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n",
    "                \n",
    "        best_model, max_val_fbeta_score = run( val_dataloader, writer)\n",
    "        \n",
    "#         if config.SAVE_BEST_ONLY:\n",
    "#             if fold_best_val_fbeta_score > max_val_fbeta_score:\n",
    "#                 best_model = fold_best_model\n",
    "#                 max_val_fbeta_score = fold_best_val_fbeta_score\n",
    "                \n",
    "#                 model_name = 'scibert_input_best_model'\n",
    "#                 torch.save(best_model.state_dict(), model_name+'.pt')\n",
    "        \n",
    "        writer.close()\n",
    "                \n",
    "    return best_model, max_val_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = config.DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= fold: 0 training =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PubMedBert:\n\tMissing key(s) in state_dict: \"model.embeddings.position_ids\", \"model.embeddings.word_embeddings.weight\", \"model.embeddings.position_embeddings.weight\", \"model.embeddings.token_type_embeddings.weight\", \"model.embeddings.LayerNorm.weight\", \"model.embeddings.LayerNorm.bias\", \"model.encoder.layer.0.attention.self.query.weight\", \"model.encoder.layer.0.attention.self.query.bias\", \"model.encoder.layer.0.attention.self.key.weight\", \"model.encoder.layer.0.attention.self.key.bias\", \"model.encoder.layer.0.attention.self.value.weight\", \"model.encoder.layer.0.attention.self.value.bias\", \"model.encoder.layer.0.attention.output.dense.weight\", \"model.encoder.layer.0.attention.output.dense.bias\", \"model.encoder.layer.0.attention.output.LayerNorm.weight\", \"model.encoder.layer.0.attention.output.LayerNorm.bias\", \"model.encoder.layer.0.intermediate.dense.weight\", \"model.encoder.layer.0.intermediate.dense.bias\", \"model.encoder.layer.0.output.dense.weight\", \"model.encoder.layer.0.output.dense.bias\", \"model.encoder.layer.0.output.LayerNorm.weight\", \"model.encoder.layer.0.output.LayerNorm.bias\", \"model.encoder.layer.1.attention.self.query.weight\", \"model.encoder.layer.1.attention.self.query.bias\", \"model.encoder.layer.1.attention.self.key.weight\", \"model.encoder.layer.1.attention.self.key.bias\", \"model.encoder.layer.1.attention.self.value.weight\", \"model.encoder.layer.1.attention.self.value.bias\", \"model.encoder.layer.1.attention.output.dense.weight\", \"model.encoder.layer.1.attention.output.dense.bias\", \"model.encoder.layer.1.attention.output.LayerNorm.weight\", \"model.encoder.layer.1.attention.output.LayerNorm.bias\", \"model.encoder.layer.1.intermediate.dense.weight\", \"model.encoder.layer.1.intermediate.dense.bias\", \"model.encoder.layer.1.output.dense.weight\", \"model.encoder.layer.1.output.dense.bias\", \"model.encoder.layer.1.output.LayerNorm.weight\", \"model.encoder.layer.1.output.LayerNorm.bias\", \"model.encoder.layer.2.attention.self.query.weight\", \"model.encoder.layer.2.attention.self.query.bias\", \"model.encoder.layer.2.attention.self.key.weight\", \"model.encoder.layer.2.attention.self.key.bias\", \"model.encoder.layer.2.attention.self.value.weight\", \"model.encoder.layer.2.attention.self.value.bias\", \"model.encoder.layer.2.attention.output.dense.weight\", \"model.encoder.layer.2.attention.output.dense.bias\", \"model.encoder.layer.2.attention.output.LayerNorm.weight\", \"model.encoder.layer.2.attention.output.LayerNorm.bias\", \"model.encoder.layer.2.intermediate.dense.weight\", \"model.encoder.layer.2.intermediate.dense.bias\", \"model.encoder.layer.2.output.dense.weight\", \"model.encoder.layer.2.output.dense.bias\", \"model.encoder.layer.2.output.LayerNorm.weight\", \"model.encoder.layer.2.output.LayerNorm.bias\", \"model.encoder.layer.3.attention.self.query.weight\", \"model.encoder.layer.3.attention.self.query.bias\", \"model.encoder.layer.3.attention.self.key.weight\", \"model.encoder.layer.3.attention.self.key.bias\", \"model.encoder.layer.3.attention.self.value.weight\", \"model.encoder.layer.3.attention.self.value.bias\", \"model.encoder.layer.3.attention.output.dense.weight\", \"model.encoder.layer.3.attention.output.dense.bias\", \"model.encoder.layer.3.attention.output.LayerNorm.weight\", \"model.encoder.layer.3.attention.output.LayerNorm.bias\", \"model.encoder.layer.3.intermediate.dense.weight\", \"model.encoder.layer.3.intermediate.dense.bias\", \"model.encoder.layer.3.output.dense.weight\", \"model.encoder.layer.3.output.dense.bias\", \"model.encoder.layer.3.output.LayerNorm.weight\", \"model.encoder.layer.3.output.LayerNorm.bias\", \"model.encoder.layer.4.attention.self.query.weight\", \"model.encoder.layer.4.attention.self.query.bias\", \"model.encoder.layer.4.attention.self.key.weight\", \"model.encoder.layer.4.attention.self.key.bias\", \"model.encoder.layer.4.attention.self.value.weight\", \"model.encoder.layer.4.attention.self.value.bias\", \"model.encoder.layer.4.attention.output.dense.weight\", \"model.encoder.layer.4.attention.output.dense.bias\", \"model.encoder.layer.4.attention.output.LayerNorm.weight\", \"model.encoder.layer.4.attention.output.LayerNorm.bias\", \"model.encoder.layer.4.intermediate.dense.weight\", \"model.encoder.layer.4.intermediate.dense.bias\", \"model.encoder.layer.4.output.dense.weight\", \"model.encoder.layer.4.output.dense.bias\", \"model.encoder.layer.4.output.LayerNorm.weight\", \"model.encoder.layer.4.output.LayerNorm.bias\", \"model.encoder.layer.5.attention.self.query.weight\", \"model.encoder.layer.5.attention.self.query.bias\", \"model.encoder.layer.5.attention.self.key.weight\", \"model.encoder.layer.5.attention.self.key.bias\", \"model.encoder.layer.5.attention.self.value.weight\", \"model.encoder.layer.5.attention.self.value.bias\", \"model.encoder.layer.5.attention.output.dense.weight\", \"model.encoder.layer.5.attention.output.dense.bias\", \"model.encoder.layer.5.attention.output.LayerNorm.weight\", \"model.encoder.layer.5.attention.output.LayerNorm.bias\", \"model.encoder.layer.5.intermediate.dense.weight\", \"model.encoder.layer.5.intermediate.dense.bias\", \"model.encoder.layer.5.output.dense.weight\", \"model.encoder.layer.5.output.dense.bias\", \"model.encoder.layer.5.output.LayerNorm.weight\", \"model.encoder.layer.5.output.LayerNorm.bias\", \"model.encoder.layer.6.attention.self.query.weight\", \"model.encoder.layer.6.attention.self.query.bias\", \"model.encoder.layer.6.attention.self.key.weight\", \"model.encoder.layer.6.attention.self.key.bias\", \"model.encoder.layer.6.attention.self.value.weight\", \"model.encoder.layer.6.attention.self.value.bias\", \"model.encoder.layer.6.attention.output.dense.weight\", \"model.encoder.layer.6.attention.output.dense.bias\", \"model.encoder.layer.6.attention.output.LayerNorm.weight\", \"model.encoder.layer.6.attention.output.LayerNorm.bias\", \"model.encoder.layer.6.intermediate.dense.weight\", \"model.encoder.layer.6.intermediate.dense.bias\", \"model.encoder.layer.6.output.dense.weight\", \"model.encoder.layer.6.output.dense.bias\", \"model.encoder.layer.6.output.LayerNorm.weight\", \"model.encoder.layer.6.output.LayerNorm.bias\", \"model.encoder.layer.7.attention.self.query.weight\", \"model.encoder.layer.7.attention.self.query.bias\", \"model.encoder.layer.7.attention.self.key.weight\", \"model.encoder.layer.7.attention.self.key.bias\", \"model.encoder.layer.7.attention.self.value.weight\", \"model.encoder.layer.7.attention.self.value.bias\", \"model.encoder.layer.7.attention.output.dense.weight\", \"model.encoder.layer.7.attention.output.dense.bias\", \"model.encoder.layer.7.attention.output.LayerNorm.weight\", \"model.encoder.layer.7.attention.output.LayerNorm.bias\", \"model.encoder.layer.7.intermediate.dense.weight\", \"model.encoder.layer.7.intermediate.dense.bias\", \"model.encoder.layer.7.output.dense.weight\", \"model.encoder.layer.7.output.dense.bias\", \"model.encoder.layer.7.output.LayerNorm.weight\", \"model.encoder.layer.7.output.LayerNorm.bias\", \"model.encoder.layer.8.attention.self.query.weight\", \"model.encoder.layer.8.attention.self.query.bias\", \"model.encoder.layer.8.attention.self.key.weight\", \"model.encoder.layer.8.attention.self.key.bias\", \"model.encoder.layer.8.attention.self.value.weight\", \"model.encoder.layer.8.attention.self.value.bias\", \"model.encoder.layer.8.attention.output.dense.weight\", \"model.encoder.layer.8.attention.output.dense.bias\", \"model.encoder.layer.8.attention.output.LayerNorm.weight\", \"model.encoder.layer.8.attention.output.LayerNorm.bias\", \"model.encoder.layer.8.intermediate.dense.weight\", \"model.encoder.layer.8.intermediate.dense.bias\", \"model.encoder.layer.8.output.dense.weight\", \"model.encoder.layer.8.output.dense.bias\", \"model.encoder.layer.8.output.LayerNorm.weight\", \"model.encoder.layer.8.output.LayerNorm.bias\", \"model.encoder.layer.9.attention.self.query.weight\", \"model.encoder.layer.9.attention.self.query.bias\", \"model.encoder.layer.9.attention.self.key.weight\", \"model.encoder.layer.9.attention.self.key.bias\", \"model.encoder.layer.9.attention.self.value.weight\", \"model.encoder.layer.9.attention.self.value.bias\", \"model.encoder.layer.9.attention.output.dense.weight\", \"model.encoder.layer.9.attention.output.dense.bias\", \"model.encoder.layer.9.attention.output.LayerNorm.weight\", \"model.encoder.layer.9.attention.output.LayerNorm.bias\", \"model.encoder.layer.9.intermediate.dense.weight\", \"model.encoder.layer.9.intermediate.dense.bias\", \"model.encoder.layer.9.output.dense.weight\", \"model.encoder.layer.9.output.dense.bias\", \"model.encoder.layer.9.output.LayerNorm.weight\", \"model.encoder.layer.9.output.LayerNorm.bias\", \"model.encoder.layer.10.attention.self.query.weight\", \"model.encoder.layer.10.attention.self.query.bias\", \"model.encoder.layer.10.attention.self.key.weight\", \"model.encoder.layer.10.attention.self.key.bias\", \"model.encoder.layer.10.attention.self.value.weight\", \"model.encoder.layer.10.attention.self.value.bias\", \"model.encoder.layer.10.attention.output.dense.weight\", \"model.encoder.layer.10.attention.output.dense.bias\", \"model.encoder.layer.10.attention.output.LayerNorm.weight\", \"model.encoder.layer.10.attention.output.LayerNorm.bias\", \"model.encoder.layer.10.intermediate.dense.weight\", \"model.encoder.layer.10.intermediate.dense.bias\", \"model.encoder.layer.10.output.dense.weight\", \"model.encoder.layer.10.output.dense.bias\", \"model.encoder.layer.10.output.LayerNorm.weight\", \"model.encoder.layer.10.output.LayerNorm.bias\", \"model.encoder.layer.11.attention.self.query.weight\", \"model.encoder.layer.11.attention.self.query.bias\", \"model.encoder.layer.11.attention.self.key.weight\", \"model.encoder.layer.11.attention.self.key.bias\", \"model.encoder.layer.11.attention.self.value.weight\", \"model.encoder.layer.11.attention.self.value.bias\", \"model.encoder.layer.11.attention.output.dense.weight\", \"model.encoder.layer.11.attention.output.dense.bias\", \"model.encoder.layer.11.attention.output.LayerNorm.weight\", \"model.encoder.layer.11.attention.output.LayerNorm.bias\", \"model.encoder.layer.11.intermediate.dense.weight\", \"model.encoder.layer.11.intermediate.dense.bias\", \"model.encoder.layer.11.output.dense.weight\", \"model.encoder.layer.11.output.dense.bias\", \"model.encoder.layer.11.output.LayerNorm.weight\", \"model.encoder.layer.11.output.LayerNorm.bias\", \"model.pooler.dense.weight\", \"model.pooler.dense.bias\", \"linear.weight\", \"linear.bias\". \n\tUnexpected key(s) in state_dict: \"module.model.embeddings.position_ids\", \"module.model.embeddings.word_embeddings.weight\", \"module.model.embeddings.position_embeddings.weight\", \"module.model.embeddings.token_type_embeddings.weight\", \"module.model.embeddings.LayerNorm.weight\", \"module.model.embeddings.LayerNorm.bias\", \"module.model.encoder.layer.0.attention.self.query.weight\", \"module.model.encoder.layer.0.attention.self.query.bias\", \"module.model.encoder.layer.0.attention.self.key.weight\", \"module.model.encoder.layer.0.attention.self.key.bias\", \"module.model.encoder.layer.0.attention.self.value.weight\", \"module.model.encoder.layer.0.attention.self.value.bias\", \"module.model.encoder.layer.0.attention.output.dense.weight\", \"module.model.encoder.layer.0.attention.output.dense.bias\", \"module.model.encoder.layer.0.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.0.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.0.intermediate.dense.weight\", \"module.model.encoder.layer.0.intermediate.dense.bias\", \"module.model.encoder.layer.0.output.dense.weight\", \"module.model.encoder.layer.0.output.dense.bias\", \"module.model.encoder.layer.0.output.LayerNorm.weight\", \"module.model.encoder.layer.0.output.LayerNorm.bias\", \"module.model.encoder.layer.1.attention.self.query.weight\", \"module.model.encoder.layer.1.attention.self.query.bias\", \"module.model.encoder.layer.1.attention.self.key.weight\", \"module.model.encoder.layer.1.attention.self.key.bias\", \"module.model.encoder.layer.1.attention.self.value.weight\", \"module.model.encoder.layer.1.attention.self.value.bias\", \"module.model.encoder.layer.1.attention.output.dense.weight\", \"module.model.encoder.layer.1.attention.output.dense.bias\", \"module.model.encoder.layer.1.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.1.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.1.intermediate.dense.weight\", \"module.model.encoder.layer.1.intermediate.dense.bias\", \"module.model.encoder.layer.1.output.dense.weight\", \"module.model.encoder.layer.1.output.dense.bias\", \"module.model.encoder.layer.1.output.LayerNorm.weight\", \"module.model.encoder.layer.1.output.LayerNorm.bias\", \"module.model.encoder.layer.2.attention.self.query.weight\", \"module.model.encoder.layer.2.attention.self.query.bias\", \"module.model.encoder.layer.2.attention.self.key.weight\", \"module.model.encoder.layer.2.attention.self.key.bias\", \"module.model.encoder.layer.2.attention.self.value.weight\", \"module.model.encoder.layer.2.attention.self.value.bias\", \"module.model.encoder.layer.2.attention.output.dense.weight\", \"module.model.encoder.layer.2.attention.output.dense.bias\", \"module.model.encoder.layer.2.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.2.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.2.intermediate.dense.weight\", \"module.model.encoder.layer.2.intermediate.dense.bias\", \"module.model.encoder.layer.2.output.dense.weight\", \"module.model.encoder.layer.2.output.dense.bias\", \"module.model.encoder.layer.2.output.LayerNorm.weight\", \"module.model.encoder.layer.2.output.LayerNorm.bias\", \"module.model.encoder.layer.3.attention.self.query.weight\", \"module.model.encoder.layer.3.attention.self.query.bias\", \"module.model.encoder.layer.3.attention.self.key.weight\", \"module.model.encoder.layer.3.attention.self.key.bias\", \"module.model.encoder.layer.3.attention.self.value.weight\", \"module.model.encoder.layer.3.attention.self.value.bias\", \"module.model.encoder.layer.3.attention.output.dense.weight\", \"module.model.encoder.layer.3.attention.output.dense.bias\", \"module.model.encoder.layer.3.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.3.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.3.intermediate.dense.weight\", \"module.model.encoder.layer.3.intermediate.dense.bias\", \"module.model.encoder.layer.3.output.dense.weight\", \"module.model.encoder.layer.3.output.dense.bias\", \"module.model.encoder.layer.3.output.LayerNorm.weight\", \"module.model.encoder.layer.3.output.LayerNorm.bias\", \"module.model.encoder.layer.4.attention.self.query.weight\", \"module.model.encoder.layer.4.attention.self.query.bias\", \"module.model.encoder.layer.4.attention.self.key.weight\", \"module.model.encoder.layer.4.attention.self.key.bias\", \"module.model.encoder.layer.4.attention.self.value.weight\", \"module.model.encoder.layer.4.attention.self.value.bias\", \"module.model.encoder.layer.4.attention.output.dense.weight\", \"module.model.encoder.layer.4.attention.output.dense.bias\", \"module.model.encoder.layer.4.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.4.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.4.intermediate.dense.weight\", \"module.model.encoder.layer.4.intermediate.dense.bias\", \"module.model.encoder.layer.4.output.dense.weight\", \"module.model.encoder.layer.4.output.dense.bias\", \"module.model.encoder.layer.4.output.LayerNorm.weight\", \"module.model.encoder.layer.4.output.LayerNorm.bias\", \"module.model.encoder.layer.5.attention.self.query.weight\", \"module.model.encoder.layer.5.attention.self.query.bias\", \"module.model.encoder.layer.5.attention.self.key.weight\", \"module.model.encoder.layer.5.attention.self.key.bias\", \"module.model.encoder.layer.5.attention.self.value.weight\", \"module.model.encoder.layer.5.attention.self.value.bias\", \"module.model.encoder.layer.5.attention.output.dense.weight\", \"module.model.encoder.layer.5.attention.output.dense.bias\", \"module.model.encoder.layer.5.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.5.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.5.intermediate.dense.weight\", \"module.model.encoder.layer.5.intermediate.dense.bias\", \"module.model.encoder.layer.5.output.dense.weight\", \"module.model.encoder.layer.5.output.dense.bias\", \"module.model.encoder.layer.5.output.LayerNorm.weight\", \"module.model.encoder.layer.5.output.LayerNorm.bias\", \"module.model.encoder.layer.6.attention.self.query.weight\", \"module.model.encoder.layer.6.attention.self.query.bias\", \"module.model.encoder.layer.6.attention.self.key.weight\", \"module.model.encoder.layer.6.attention.self.key.bias\", \"module.model.encoder.layer.6.attention.self.value.weight\", \"module.model.encoder.layer.6.attention.self.value.bias\", \"module.model.encoder.layer.6.attention.output.dense.weight\", \"module.model.encoder.layer.6.attention.output.dense.bias\", \"module.model.encoder.layer.6.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.6.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.6.intermediate.dense.weight\", \"module.model.encoder.layer.6.intermediate.dense.bias\", \"module.model.encoder.layer.6.output.dense.weight\", \"module.model.encoder.layer.6.output.dense.bias\", \"module.model.encoder.layer.6.output.LayerNorm.weight\", \"module.model.encoder.layer.6.output.LayerNorm.bias\", \"module.model.encoder.layer.7.attention.self.query.weight\", \"module.model.encoder.layer.7.attention.self.query.bias\", \"module.model.encoder.layer.7.attention.self.key.weight\", \"module.model.encoder.layer.7.attention.self.key.bias\", \"module.model.encoder.layer.7.attention.self.value.weight\", \"module.model.encoder.layer.7.attention.self.value.bias\", \"module.model.encoder.layer.7.attention.output.dense.weight\", \"module.model.encoder.layer.7.attention.output.dense.bias\", \"module.model.encoder.layer.7.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.7.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.7.intermediate.dense.weight\", \"module.model.encoder.layer.7.intermediate.dense.bias\", \"module.model.encoder.layer.7.output.dense.weight\", \"module.model.encoder.layer.7.output.dense.bias\", \"module.model.encoder.layer.7.output.LayerNorm.weight\", \"module.model.encoder.layer.7.output.LayerNorm.bias\", \"module.model.encoder.layer.8.attention.self.query.weight\", \"module.model.encoder.layer.8.attention.self.query.bias\", \"module.model.encoder.layer.8.attention.self.key.weight\", \"module.model.encoder.layer.8.attention.self.key.bias\", \"module.model.encoder.layer.8.attention.self.value.weight\", \"module.model.encoder.layer.8.attention.self.value.bias\", \"module.model.encoder.layer.8.attention.output.dense.weight\", \"module.model.encoder.layer.8.attention.output.dense.bias\", \"module.model.encoder.layer.8.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.8.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.8.intermediate.dense.weight\", \"module.model.encoder.layer.8.intermediate.dense.bias\", \"module.model.encoder.layer.8.output.dense.weight\", \"module.model.encoder.layer.8.output.dense.bias\", \"module.model.encoder.layer.8.output.LayerNorm.weight\", \"module.model.encoder.layer.8.output.LayerNorm.bias\", \"module.model.encoder.layer.9.attention.self.query.weight\", \"module.model.encoder.layer.9.attention.self.query.bias\", \"module.model.encoder.layer.9.attention.self.key.weight\", \"module.model.encoder.layer.9.attention.self.key.bias\", \"module.model.encoder.layer.9.attention.self.value.weight\", \"module.model.encoder.layer.9.attention.self.value.bias\", \"module.model.encoder.layer.9.attention.output.dense.weight\", \"module.model.encoder.layer.9.attention.output.dense.bias\", \"module.model.encoder.layer.9.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.9.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.9.intermediate.dense.weight\", \"module.model.encoder.layer.9.intermediate.dense.bias\", \"module.model.encoder.layer.9.output.dense.weight\", \"module.model.encoder.layer.9.output.dense.bias\", \"module.model.encoder.layer.9.output.LayerNorm.weight\", \"module.model.encoder.layer.9.output.LayerNorm.bias\", \"module.model.encoder.layer.10.attention.self.query.weight\", \"module.model.encoder.layer.10.attention.self.query.bias\", \"module.model.encoder.layer.10.attention.self.key.weight\", \"module.model.encoder.layer.10.attention.self.key.bias\", \"module.model.encoder.layer.10.attention.self.value.weight\", \"module.model.encoder.layer.10.attention.self.value.bias\", \"module.model.encoder.layer.10.attention.output.dense.weight\", \"module.model.encoder.layer.10.attention.output.dense.bias\", \"module.model.encoder.layer.10.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.10.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.10.intermediate.dense.weight\", \"module.model.encoder.layer.10.intermediate.dense.bias\", \"module.model.encoder.layer.10.output.dense.weight\", \"module.model.encoder.layer.10.output.dense.bias\", \"module.model.encoder.layer.10.output.LayerNorm.weight\", \"module.model.encoder.layer.10.output.LayerNorm.bias\", \"module.model.encoder.layer.11.attention.self.query.weight\", \"module.model.encoder.layer.11.attention.self.query.bias\", \"module.model.encoder.layer.11.attention.self.key.weight\", \"module.model.encoder.layer.11.attention.self.key.bias\", \"module.model.encoder.layer.11.attention.self.value.weight\", \"module.model.encoder.layer.11.attention.self.value.bias\", \"module.model.encoder.layer.11.attention.output.dense.weight\", \"module.model.encoder.layer.11.attention.output.dense.bias\", \"module.model.encoder.layer.11.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.11.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.11.intermediate.dense.weight\", \"module.model.encoder.layer.11.intermediate.dense.bias\", \"module.model.encoder.layer.11.output.dense.weight\", \"module.model.encoder.layer.11.output.dense.bias\", \"module.model.encoder.layer.11.output.LayerNorm.weight\", \"module.model.encoder.layer.11.output.LayerNorm.bias\", \"module.model.pooler.dense.weight\", \"module.model.pooler.dense.bias\", \"module.linear.weight\", \"module.linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1793778/588164985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_fbeta_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1793778/1711437873.py\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_fbeta_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         if config.SAVE_BEST_ONLY:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1793778/3307431197.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(val_dataloader, writer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPubMedBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emilyalsentzer/Bio_ClinicalBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pubmedbert_input_best_model_biomed.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/berry/home/prakhar/signateGithub/signate_471/abc/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PubMedBert:\n\tMissing key(s) in state_dict: \"model.embeddings.position_ids\", \"model.embeddings.word_embeddings.weight\", \"model.embeddings.position_embeddings.weight\", \"model.embeddings.token_type_embeddings.weight\", \"model.embeddings.LayerNorm.weight\", \"model.embeddings.LayerNorm.bias\", \"model.encoder.layer.0.attention.self.query.weight\", \"model.encoder.layer.0.attention.self.query.bias\", \"model.encoder.layer.0.attention.self.key.weight\", \"model.encoder.layer.0.attention.self.key.bias\", \"model.encoder.layer.0.attention.self.value.weight\", \"model.encoder.layer.0.attention.self.value.bias\", \"model.encoder.layer.0.attention.output.dense.weight\", \"model.encoder.layer.0.attention.output.dense.bias\", \"model.encoder.layer.0.attention.output.LayerNorm.weight\", \"model.encoder.layer.0.attention.output.LayerNorm.bias\", \"model.encoder.layer.0.intermediate.dense.weight\", \"model.encoder.layer.0.intermediate.dense.bias\", \"model.encoder.layer.0.output.dense.weight\", \"model.encoder.layer.0.output.dense.bias\", \"model.encoder.layer.0.output.LayerNorm.weight\", \"model.encoder.layer.0.output.LayerNorm.bias\", \"model.encoder.layer.1.attention.self.query.weight\", \"model.encoder.layer.1.attention.self.query.bias\", \"model.encoder.layer.1.attention.self.key.weight\", \"model.encoder.layer.1.attention.self.key.bias\", \"model.encoder.layer.1.attention.self.value.weight\", \"model.encoder.layer.1.attention.self.value.bias\", \"model.encoder.layer.1.attention.output.dense.weight\", \"model.encoder.layer.1.attention.output.dense.bias\", \"model.encoder.layer.1.attention.output.LayerNorm.weight\", \"model.encoder.layer.1.attention.output.LayerNorm.bias\", \"model.encoder.layer.1.intermediate.dense.weight\", \"model.encoder.layer.1.intermediate.dense.bias\", \"model.encoder.layer.1.output.dense.weight\", \"model.encoder.layer.1.output.dense.bias\", \"model.encoder.layer.1.output.LayerNorm.weight\", \"model.encoder.layer.1.output.LayerNorm.bias\", \"model.encoder.layer.2.attention.self.query.weight\", \"model.encoder.layer.2.attention.self.query.bias\", \"model.encoder.layer.2.attention.self.key.weight\", \"model.encoder.layer.2.attention.self.key.bias\", \"model.encoder.layer.2.attention.self.value.weight\", \"model.encoder.layer.2.attention.self.value.bias\", \"model.encoder.layer.2.attention.output.dense.weight\", \"model.encoder.layer.2.attention.output.dense.bias\", \"model.encoder.layer.2.attention.output.LayerNorm.weight\", \"model.encoder.layer.2.attention.output.LayerNorm.bias\", \"model.encoder.layer.2.intermediate.dense.weight\", \"model.encoder.layer.2.intermediate.dense.bias\", \"model.encoder.layer.2.output.dense.weight\", \"model.encoder.layer.2.output.dense.bias\", \"model.encoder.layer.2.output.LayerNorm.weight\", \"model.encoder.layer.2.output.LayerNorm.bias\", \"model.encoder.layer.3.attention.self.query.weight\", \"model.encoder.layer.3.attention.self.query.bias\", \"model.encoder.layer.3.attention.self.key.weight\", \"model.encoder.layer.3.attention.self.key.bias\", \"model.encoder.layer.3.attention.self.value.weight\", \"model.encoder.layer.3.attention.self.value.bias\", \"model.encoder.layer.3.attention.output.dense.weight\", \"model.encoder.layer.3.attention.output.dense.bias\", \"model.encoder.layer.3.attention.output.LayerNorm.weight\", \"model.encoder.layer.3.attention.output.LayerNorm.bias\", \"model.encoder.layer.3.intermediate.dense.weight\", \"model.encoder.layer.3.intermediate.dense.bias\", \"model.encoder.layer.3.output.dense.weight\", \"model.encoder.layer.3.output.dense.bias\", \"model.encoder.layer.3.output.LayerNorm.weight\", \"model.encoder.layer.3.output.LayerNorm.bias\", \"model.encoder.layer.4.attention.self.query.weight\", \"model.encoder.layer.4.attention.self.query.bias\", \"model.encoder.layer.4.attention.self.key.weight\", \"model.encoder.layer.4.attention.self.key.bias\", \"model.encoder.layer.4.attention.self.value.weight\", \"model.encoder.layer.4.attention.self.value.bias\", \"model.encoder.layer.4.attention.output.dense.weight\", \"model.encoder.layer.4.attention.output.dense.bias\", \"model.encoder.layer.4.attention.output.LayerNorm.weight\", \"model.encoder.layer.4.attention.output.LayerNorm.bias\", \"model.encoder.layer.4.intermediate.dense.weight\", \"model.encoder.layer.4.intermediate.dense.bias\", \"model.encoder.layer.4.output.dense.weight\", \"model.encoder.layer.4.output.dense.bias\", \"model.encoder.layer.4.output.LayerNorm.weight\", \"model.encoder.layer.4.output.LayerNorm.bias\", \"model.encoder.layer.5.attention.self.query.weight\", \"model.encoder.layer.5.attention.self.query.bias\", \"model.encoder.layer.5.attention.self.key.weight\", \"model.encoder.layer.5.attention.self.key.bias\", \"model.encoder.layer.5.attention.self.value.weight\", \"model.encoder.layer.5.attention.self.value.bias\", \"model.encoder.layer.5.attention.output.dense.weight\", \"model.encoder.layer.5.attention.output.dense.bias\", \"model.encoder.layer.5.attention.output.LayerNorm.weight\", \"model.encoder.layer.5.attention.output.LayerNorm.bias\", \"model.encoder.layer.5.intermediate.dense.weight\", \"model.encoder.layer.5.intermediate.dense.bias\", \"model.encoder.layer.5.output.dense.weight\", \"model.encoder.layer.5.output.dense.bias\", \"model.encoder.layer.5.output.LayerNorm.weight\", \"model.encoder.layer.5.output.LayerNorm.bias\", \"model.encoder.layer.6.attention.self.query.weight\", \"model.encoder.layer.6.attention.self.query.bias\", \"model.encoder.layer.6.attention.self.key.weight\", \"model.encoder.layer.6.attention.self.key.bias\", \"model.encoder.layer.6.attention.self.value.weight\", \"model.encoder.layer.6.attention.self.value.bias\", \"model.encoder.layer.6.attention.output.dense.weight\", \"model.encoder.layer.6.attention.output.dense.bias\", \"model.encoder.layer.6.attention.output.LayerNorm.weight\", \"model.encoder.layer.6.attention.output.LayerNorm.bias\", \"model.encoder.layer.6.intermediate.dense.weight\", \"model.encoder.layer.6.intermediate.dense.bias\", \"model.encoder.layer.6.output.dense.weight\", \"model.encoder.layer.6.output.dense.bias\", \"model.encoder.layer.6.output.LayerNorm.weight\", \"model.encoder.layer.6.output.LayerNorm.bias\", \"model.encoder.layer.7.attention.self.query.weight\", \"model.encoder.layer.7.attention.self.query.bias\", \"model.encoder.layer.7.attention.self.key.weight\", \"model.encoder.layer.7.attention.self.key.bias\", \"model.encoder.layer.7.attention.self.value.weight\", \"model.encoder.layer.7.attention.self.value.bias\", \"model.encoder.layer.7.attention.output.dense.weight\", \"model.encoder.layer.7.attention.output.dense.bias\", \"model.encoder.layer.7.attention.output.LayerNorm.weight\", \"model.encoder.layer.7.attention.output.LayerNorm.bias\", \"model.encoder.layer.7.intermediate.dense.weight\", \"model.encoder.layer.7.intermediate.dense.bias\", \"model.encoder.layer.7.output.dense.weight\", \"model.encoder.layer.7.output.dense.bias\", \"model.encoder.layer.7.output.LayerNorm.weight\", \"model.encoder.layer.7.output.LayerNorm.bias\", \"model.encoder.layer.8.attention.self.query.weight\", \"model.encoder.layer.8.attention.self.query.bias\", \"model.encoder.layer.8.attention.self.key.weight\", \"model.encoder.layer.8.attention.self.key.bias\", \"model.encoder.layer.8.attention.self.value.weight\", \"model.encoder.layer.8.attention.self.value.bias\", \"model.encoder.layer.8.attention.output.dense.weight\", \"model.encoder.layer.8.attention.output.dense.bias\", \"model.encoder.layer.8.attention.output.LayerNorm.weight\", \"model.encoder.layer.8.attention.output.LayerNorm.bias\", \"model.encoder.layer.8.intermediate.dense.weight\", \"model.encoder.layer.8.intermediate.dense.bias\", \"model.encoder.layer.8.output.dense.weight\", \"model.encoder.layer.8.output.dense.bias\", \"model.encoder.layer.8.output.LayerNorm.weight\", \"model.encoder.layer.8.output.LayerNorm.bias\", \"model.encoder.layer.9.attention.self.query.weight\", \"model.encoder.layer.9.attention.self.query.bias\", \"model.encoder.layer.9.attention.self.key.weight\", \"model.encoder.layer.9.attention.self.key.bias\", \"model.encoder.layer.9.attention.self.value.weight\", \"model.encoder.layer.9.attention.self.value.bias\", \"model.encoder.layer.9.attention.output.dense.weight\", \"model.encoder.layer.9.attention.output.dense.bias\", \"model.encoder.layer.9.attention.output.LayerNorm.weight\", \"model.encoder.layer.9.attention.output.LayerNorm.bias\", \"model.encoder.layer.9.intermediate.dense.weight\", \"model.encoder.layer.9.intermediate.dense.bias\", \"model.encoder.layer.9.output.dense.weight\", \"model.encoder.layer.9.output.dense.bias\", \"model.encoder.layer.9.output.LayerNorm.weight\", \"model.encoder.layer.9.output.LayerNorm.bias\", \"model.encoder.layer.10.attention.self.query.weight\", \"model.encoder.layer.10.attention.self.query.bias\", \"model.encoder.layer.10.attention.self.key.weight\", \"model.encoder.layer.10.attention.self.key.bias\", \"model.encoder.layer.10.attention.self.value.weight\", \"model.encoder.layer.10.attention.self.value.bias\", \"model.encoder.layer.10.attention.output.dense.weight\", \"model.encoder.layer.10.attention.output.dense.bias\", \"model.encoder.layer.10.attention.output.LayerNorm.weight\", \"model.encoder.layer.10.attention.output.LayerNorm.bias\", \"model.encoder.layer.10.intermediate.dense.weight\", \"model.encoder.layer.10.intermediate.dense.bias\", \"model.encoder.layer.10.output.dense.weight\", \"model.encoder.layer.10.output.dense.bias\", \"model.encoder.layer.10.output.LayerNorm.weight\", \"model.encoder.layer.10.output.LayerNorm.bias\", \"model.encoder.layer.11.attention.self.query.weight\", \"model.encoder.layer.11.attention.self.query.bias\", \"model.encoder.layer.11.attention.self.key.weight\", \"model.encoder.layer.11.attention.self.key.bias\", \"model.encoder.layer.11.attention.self.value.weight\", \"model.encoder.layer.11.attention.self.value.bias\", \"model.encoder.layer.11.attention.output.dense.weight\", \"model.encoder.layer.11.attention.output.dense.bias\", \"model.encoder.layer.11.attention.output.LayerNorm.weight\", \"model.encoder.layer.11.attention.output.LayerNorm.bias\", \"model.encoder.layer.11.intermediate.dense.weight\", \"model.encoder.layer.11.intermediate.dense.bias\", \"model.encoder.layer.11.output.dense.weight\", \"model.encoder.layer.11.output.dense.bias\", \"model.encoder.layer.11.output.LayerNorm.weight\", \"model.encoder.layer.11.output.LayerNorm.bias\", \"model.pooler.dense.weight\", \"model.pooler.dense.bias\", \"linear.weight\", \"linear.bias\". \n\tUnexpected key(s) in state_dict: \"module.model.embeddings.position_ids\", \"module.model.embeddings.word_embeddings.weight\", \"module.model.embeddings.position_embeddings.weight\", \"module.model.embeddings.token_type_embeddings.weight\", \"module.model.embeddings.LayerNorm.weight\", \"module.model.embeddings.LayerNorm.bias\", \"module.model.encoder.layer.0.attention.self.query.weight\", \"module.model.encoder.layer.0.attention.self.query.bias\", \"module.model.encoder.layer.0.attention.self.key.weight\", \"module.model.encoder.layer.0.attention.self.key.bias\", \"module.model.encoder.layer.0.attention.self.value.weight\", \"module.model.encoder.layer.0.attention.self.value.bias\", \"module.model.encoder.layer.0.attention.output.dense.weight\", \"module.model.encoder.layer.0.attention.output.dense.bias\", \"module.model.encoder.layer.0.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.0.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.0.intermediate.dense.weight\", \"module.model.encoder.layer.0.intermediate.dense.bias\", \"module.model.encoder.layer.0.output.dense.weight\", \"module.model.encoder.layer.0.output.dense.bias\", \"module.model.encoder.layer.0.output.LayerNorm.weight\", \"module.model.encoder.layer.0.output.LayerNorm.bias\", \"module.model.encoder.layer.1.attention.self.query.weight\", \"module.model.encoder.layer.1.attention.self.query.bias\", \"module.model.encoder.layer.1.attention.self.key.weight\", \"module.model.encoder.layer.1.attention.self.key.bias\", \"module.model.encoder.layer.1.attention.self.value.weight\", \"module.model.encoder.layer.1.attention.self.value.bias\", \"module.model.encoder.layer.1.attention.output.dense.weight\", \"module.model.encoder.layer.1.attention.output.dense.bias\", \"module.model.encoder.layer.1.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.1.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.1.intermediate.dense.weight\", \"module.model.encoder.layer.1.intermediate.dense.bias\", \"module.model.encoder.layer.1.output.dense.weight\", \"module.model.encoder.layer.1.output.dense.bias\", \"module.model.encoder.layer.1.output.LayerNorm.weight\", \"module.model.encoder.layer.1.output.LayerNorm.bias\", \"module.model.encoder.layer.2.attention.self.query.weight\", \"module.model.encoder.layer.2.attention.self.query.bias\", \"module.model.encoder.layer.2.attention.self.key.weight\", \"module.model.encoder.layer.2.attention.self.key.bias\", \"module.model.encoder.layer.2.attention.self.value.weight\", \"module.model.encoder.layer.2.attention.self.value.bias\", \"module.model.encoder.layer.2.attention.output.dense.weight\", \"module.model.encoder.layer.2.attention.output.dense.bias\", \"module.model.encoder.layer.2.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.2.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.2.intermediate.dense.weight\", \"module.model.encoder.layer.2.intermediate.dense.bias\", \"module.model.encoder.layer.2.output.dense.weight\", \"module.model.encoder.layer.2.output.dense.bias\", \"module.model.encoder.layer.2.output.LayerNorm.weight\", \"module.model.encoder.layer.2.output.LayerNorm.bias\", \"module.model.encoder.layer.3.attention.self.query.weight\", \"module.model.encoder.layer.3.attention.self.query.bias\", \"module.model.encoder.layer.3.attention.self.key.weight\", \"module.model.encoder.layer.3.attention.self.key.bias\", \"module.model.encoder.layer.3.attention.self.value.weight\", \"module.model.encoder.layer.3.attention.self.value.bias\", \"module.model.encoder.layer.3.attention.output.dense.weight\", \"module.model.encoder.layer.3.attention.output.dense.bias\", \"module.model.encoder.layer.3.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.3.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.3.intermediate.dense.weight\", \"module.model.encoder.layer.3.intermediate.dense.bias\", \"module.model.encoder.layer.3.output.dense.weight\", \"module.model.encoder.layer.3.output.dense.bias\", \"module.model.encoder.layer.3.output.LayerNorm.weight\", \"module.model.encoder.layer.3.output.LayerNorm.bias\", \"module.model.encoder.layer.4.attention.self.query.weight\", \"module.model.encoder.layer.4.attention.self.query.bias\", \"module.model.encoder.layer.4.attention.self.key.weight\", \"module.model.encoder.layer.4.attention.self.key.bias\", \"module.model.encoder.layer.4.attention.self.value.weight\", \"module.model.encoder.layer.4.attention.self.value.bias\", \"module.model.encoder.layer.4.attention.output.dense.weight\", \"module.model.encoder.layer.4.attention.output.dense.bias\", \"module.model.encoder.layer.4.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.4.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.4.intermediate.dense.weight\", \"module.model.encoder.layer.4.intermediate.dense.bias\", \"module.model.encoder.layer.4.output.dense.weight\", \"module.model.encoder.layer.4.output.dense.bias\", \"module.model.encoder.layer.4.output.LayerNorm.weight\", \"module.model.encoder.layer.4.output.LayerNorm.bias\", \"module.model.encoder.layer.5.attention.self.query.weight\", \"module.model.encoder.layer.5.attention.self.query.bias\", \"module.model.encoder.layer.5.attention.self.key.weight\", \"module.model.encoder.layer.5.attention.self.key.bias\", \"module.model.encoder.layer.5.attention.self.value.weight\", \"module.model.encoder.layer.5.attention.self.value.bias\", \"module.model.encoder.layer.5.attention.output.dense.weight\", \"module.model.encoder.layer.5.attention.output.dense.bias\", \"module.model.encoder.layer.5.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.5.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.5.intermediate.dense.weight\", \"module.model.encoder.layer.5.intermediate.dense.bias\", \"module.model.encoder.layer.5.output.dense.weight\", \"module.model.encoder.layer.5.output.dense.bias\", \"module.model.encoder.layer.5.output.LayerNorm.weight\", \"module.model.encoder.layer.5.output.LayerNorm.bias\", \"module.model.encoder.layer.6.attention.self.query.weight\", \"module.model.encoder.layer.6.attention.self.query.bias\", \"module.model.encoder.layer.6.attention.self.key.weight\", \"module.model.encoder.layer.6.attention.self.key.bias\", \"module.model.encoder.layer.6.attention.self.value.weight\", \"module.model.encoder.layer.6.attention.self.value.bias\", \"module.model.encoder.layer.6.attention.output.dense.weight\", \"module.model.encoder.layer.6.attention.output.dense.bias\", \"module.model.encoder.layer.6.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.6.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.6.intermediate.dense.weight\", \"module.model.encoder.layer.6.intermediate.dense.bias\", \"module.model.encoder.layer.6.output.dense.weight\", \"module.model.encoder.layer.6.output.dense.bias\", \"module.model.encoder.layer.6.output.LayerNorm.weight\", \"module.model.encoder.layer.6.output.LayerNorm.bias\", \"module.model.encoder.layer.7.attention.self.query.weight\", \"module.model.encoder.layer.7.attention.self.query.bias\", \"module.model.encoder.layer.7.attention.self.key.weight\", \"module.model.encoder.layer.7.attention.self.key.bias\", \"module.model.encoder.layer.7.attention.self.value.weight\", \"module.model.encoder.layer.7.attention.self.value.bias\", \"module.model.encoder.layer.7.attention.output.dense.weight\", \"module.model.encoder.layer.7.attention.output.dense.bias\", \"module.model.encoder.layer.7.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.7.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.7.intermediate.dense.weight\", \"module.model.encoder.layer.7.intermediate.dense.bias\", \"module.model.encoder.layer.7.output.dense.weight\", \"module.model.encoder.layer.7.output.dense.bias\", \"module.model.encoder.layer.7.output.LayerNorm.weight\", \"module.model.encoder.layer.7.output.LayerNorm.bias\", \"module.model.encoder.layer.8.attention.self.query.weight\", \"module.model.encoder.layer.8.attention.self.query.bias\", \"module.model.encoder.layer.8.attention.self.key.weight\", \"module.model.encoder.layer.8.attention.self.key.bias\", \"module.model.encoder.layer.8.attention.self.value.weight\", \"module.model.encoder.layer.8.attention.self.value.bias\", \"module.model.encoder.layer.8.attention.output.dense.weight\", \"module.model.encoder.layer.8.attention.output.dense.bias\", \"module.model.encoder.layer.8.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.8.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.8.intermediate.dense.weight\", \"module.model.encoder.layer.8.intermediate.dense.bias\", \"module.model.encoder.layer.8.output.dense.weight\", \"module.model.encoder.layer.8.output.dense.bias\", \"module.model.encoder.layer.8.output.LayerNorm.weight\", \"module.model.encoder.layer.8.output.LayerNorm.bias\", \"module.model.encoder.layer.9.attention.self.query.weight\", \"module.model.encoder.layer.9.attention.self.query.bias\", \"module.model.encoder.layer.9.attention.self.key.weight\", \"module.model.encoder.layer.9.attention.self.key.bias\", \"module.model.encoder.layer.9.attention.self.value.weight\", \"module.model.encoder.layer.9.attention.self.value.bias\", \"module.model.encoder.layer.9.attention.output.dense.weight\", \"module.model.encoder.layer.9.attention.output.dense.bias\", \"module.model.encoder.layer.9.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.9.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.9.intermediate.dense.weight\", \"module.model.encoder.layer.9.intermediate.dense.bias\", \"module.model.encoder.layer.9.output.dense.weight\", \"module.model.encoder.layer.9.output.dense.bias\", \"module.model.encoder.layer.9.output.LayerNorm.weight\", \"module.model.encoder.layer.9.output.LayerNorm.bias\", \"module.model.encoder.layer.10.attention.self.query.weight\", \"module.model.encoder.layer.10.attention.self.query.bias\", \"module.model.encoder.layer.10.attention.self.key.weight\", \"module.model.encoder.layer.10.attention.self.key.bias\", \"module.model.encoder.layer.10.attention.self.value.weight\", \"module.model.encoder.layer.10.attention.self.value.bias\", \"module.model.encoder.layer.10.attention.output.dense.weight\", \"module.model.encoder.layer.10.attention.output.dense.bias\", \"module.model.encoder.layer.10.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.10.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.10.intermediate.dense.weight\", \"module.model.encoder.layer.10.intermediate.dense.bias\", \"module.model.encoder.layer.10.output.dense.weight\", \"module.model.encoder.layer.10.output.dense.bias\", \"module.model.encoder.layer.10.output.LayerNorm.weight\", \"module.model.encoder.layer.10.output.LayerNorm.bias\", \"module.model.encoder.layer.11.attention.self.query.weight\", \"module.model.encoder.layer.11.attention.self.query.bias\", \"module.model.encoder.layer.11.attention.self.key.weight\", \"module.model.encoder.layer.11.attention.self.key.bias\", \"module.model.encoder.layer.11.attention.self.value.weight\", \"module.model.encoder.layer.11.attention.self.value.bias\", \"module.model.encoder.layer.11.attention.output.dense.weight\", \"module.model.encoder.layer.11.attention.output.dense.bias\", \"module.model.encoder.layer.11.attention.output.LayerNorm.weight\", \"module.model.encoder.layer.11.attention.output.LayerNorm.bias\", \"module.model.encoder.layer.11.intermediate.dense.weight\", \"module.model.encoder.layer.11.intermediate.dense.bias\", \"module.model.encoder.layer.11.output.dense.weight\", \"module.model.encoder.layer.11.output.dense.bias\", \"module.model.encoder.layer.11.output.LayerNorm.weight\", \"module.model.encoder.layer.11.output.LayerNorm.bias\", \"module.model.pooler.dense.weight\", \"module.model.pooler.dense.bias\", \"module.linear.weight\", \"module.linear.bias\". "
     ]
    }
   ],
   "source": [
    "best_model, best_val_fbeta_score = cross_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Signate",
   "language": "python",
   "name": "signate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
