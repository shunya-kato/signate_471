{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264b07d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 20:09:53.464295: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/lib64:/usr/local/cuda/lib64:/usr/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/lib64:/usr/local/cuda/lib64:/usr/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/lib64:/usr/local/cuda/lib64:\n",
      "2021-09-28 20:09:53.464349: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboard\n",
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9967e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 28 20:10:22 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 23%   26C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  On   | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   25C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  On   | 00000000:08:00.0 Off |                  N/A |\n",
      "| 20%   27C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN V             On   | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 28%   36C    P8    25W / 250W |      0MiB / 12066MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  On   | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 20%   26C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  On   | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 20%   27C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  On   | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 25%   25C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15b3fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b12d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8c7d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "      <td>Longitudinal studies indicate that declines in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "      <td>The present study was undertaken to validate t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "      <td>Objective: To report a case series in which ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "      <td>The etiology and pathogenesis of idiopathic ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  One-year age changes in MRI brain volumes in o...   \n",
       "1   1  Supportive CSF biomarker evidence to enhance t...   \n",
       "2   2  Occurrence of basal ganglia germ cell tumors w...   \n",
       "3   3  New developments in diagnosis and therapy of C...   \n",
       "4   4  Prolonged shedding of SARS-CoV-2 in an elderly...   \n",
       "\n",
       "                                            abstract  judgement  \n",
       "0  Longitudinal studies indicate that declines in...          0  \n",
       "1  The present study was undertaken to validate t...          0  \n",
       "2  Objective: To report a case series in which ba...          0  \n",
       "3  The etiology and pathogenesis of idiopathic ch...          0  \n",
       "4                                                NaN          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b38df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "#border = len(train_df[train_df[\"judgement\"] == 1]) / len(train_df[\"judgement\"])\n",
    "border = 0.02\n",
    "print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8202a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27145"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246bb09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "title           0\n",
       "abstract     4390\n",
       "judgement       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e786e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "      <td>Longitudinal studies indicate that declines in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "      <td>The present study was undertaken to validate t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "      <td>Objective: To report a case series in which ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "      <td>The etiology and pathogenesis of idiopathic ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  One-year age changes in MRI brain volumes in o...   \n",
       "1   1  Supportive CSF biomarker evidence to enhance t...   \n",
       "2   2  Occurrence of basal ganglia germ cell tumors w...   \n",
       "3   3  New developments in diagnosis and therapy of C...   \n",
       "4   4  Prolonged shedding of SARS-CoV-2 in an elderly...   \n",
       "\n",
       "                                            abstract  judgement  \n",
       "0  Longitudinal studies indicate that declines in...          0  \n",
       "1  The present study was undertaken to validate t...          0  \n",
       "2  Objective: To report a case series in which ba...          0  \n",
       "3  The etiology and pathogenesis of idiopathic ch...          0  \n",
       "4  Prolonged shedding of SARS-CoV-2 in an elderly...          0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_texts(df):\n",
    "    titles = df['title'].values.tolist()\n",
    "    abstracts = df['abstract'].values.tolist()\n",
    "    return titles, abstracts\n",
    "\n",
    "def get_labels(df):\n",
    "    labels = df.iloc[:, 3].values\n",
    "    return labels\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    if type(text) == float:\n",
    "        text = ''\n",
    "    #print(text)\n",
    "    \n",
    "    #text = text.lower()\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [x.strip() for x in text]\n",
    "    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n",
    "    text = ' '.join(text)\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "    #text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    \n",
    "    \n",
    "    text = preprocess(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    #stop = stopwords.words('english')\n",
    "    #text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = replace_double_quotation(sentence)\n",
    "    sentence = replace_garbled_text(sentence)\n",
    "    return sentence\n",
    "\n",
    "def replace_garbled_text(sentence):\n",
    "    garbled_char_table = {\n",
    "        'Â©': '©', '–': '-', '‐': '-',\n",
    "        'ﾂ｣': '£', 'ﾂｩ': '©', 'ﾂｫ': '«', 'ﾂｮ': '®', 'ﾂｰ': '°', 'ﾂｱ': '±', 'ﾂｲ': '²', 'ﾂｳ': '³', 'ﾂｴ': '´', 'ﾂｵ': 'µ', 'ﾂｷ': '·', 'ﾂｸ': '¸', 'ﾂｹ': '¹', 'ﾂｼ': '¼', 'ﾂｽ': '½', 'ﾂｾ': '¾', 'ﾂｿ': '¿', 'ﾂ': '',\n",
    "        'ﾃｷ': '÷', 'ﾃｸ': 'ø', 'ﾃ': 'a', 'ﾃ｡': 'a', 'ﾃ｢': 'a', 'ﾃ｣': 'a', 'ﾃ､': 'a', 'ﾃ･': 'a', 'ﾃｦ': 'ae', 'ﾃｧ': 'c', 'ﾃｨ': 'e', 'ﾃｩ': 'e', 'ﾃｪ': 'e', 'ﾃｫ': 'e',\n",
    "        'ﾃｬ': 'i', 'ﾃｭ': 'i', 'ﾃｮ': 'i', 'ﾃｯ': 'i', 'ﾃｱ': 'n', 'ﾃｲ': 'o', 'ﾃｳ': 'o', 'ﾃｴ': 'o', 'ﾃｵ': 'o', 'ﾃｶ': 'o', 'ﾃｹ': 'u', 'ﾃｺ': 'u', 'ﾃｻ': 'u', 'ﾃｼ': 'u', 'ﾃｽ': 'y', 'ﾃｿ': 'y', 'ﾃ': '×', \n",
    "        'ﾎｱ': 'α', 'ﾎｲ': 'β', 'ﾎｳ': 'γ', 'ﾎｴ': 'δ', 'ﾎｵ': 'ε', 'ﾎｶ': 'ζ', 'ﾎｷ': 'η', 'ﾎｸ': 'θ', 'ﾎｹ': 'ι', 'ﾎｺ': 'κ', 'ﾎｻ': 'λ', 'ﾎｼ': 'μ', 'ﾎｽ': 'ν', 'ﾎｾ': 'ξ', 'ﾎｿ': 'ο', 'ﾎ': '',\n",
    "        'ﾏ': ' ',\n",
    "        '竕､': '≤', '竕･': '≥', '竕ｦ': '≦', '竕ｧ': '≧',\n",
    "        '窶｢': '•', '窶ｦ': '…', '窶ｲ': '′', '窶ｳ': '″', '窶ｴ': '‴', '窶': ' ',\n",
    "        '竅ｰ': '⁰', '竅ｴ': '⁴', '竅ｵ': '⁵', '竅ｶ': '⁶', '竅ｷ': '⁷', '竅ｸ': '⁸', '竅ｹ': '⁹', '竅ｺ': '⁺', '竅ｻ': '⁻', '竅ｼ': '⁼', \n",
    "        '竏･': '∥', '竏ｪ': '∪', '竏ｫ': '∫', '竏ｶ': '∶', '竏ｼ': '∼', '竏': '', \n",
    "        'ﾂ\\uf8f0': ' '\n",
    "    }\n",
    "    for garbled_char, valid_char in garbled_char_table.items():\n",
    "        sentence = sentence.replace(garbled_char, valid_char)\n",
    "    sentence = re.sub('[ぁ-んァ-ンｦ-ﾟ一-龥]', '', sentence)\n",
    "    return sentence.translate(garbled_char_table)\n",
    "\n",
    "def replace_double_quotation(sentence):\n",
    "    return sentence.replace('\"', \"'\")  \n",
    "\n",
    "def clean_data(train_df):\n",
    "    train_df.loc[train_df['abstract'].isnull(), 'abstract'] = train_df['title']\n",
    "    train_df['abstract'] = train_df['abstract'].apply(clean_text)\n",
    "    train_df['title'] = train_df['title'].apply(clean_text)\n",
    "    return train_df\n",
    "\n",
    "train_df = clean_data(train_df)\n",
    "test_df= clean_data(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c230370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title       0\n",
       "abstract    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798bb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.countplot(x='judgement', data=train_df)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = train_df['title'].tolist()\n",
    "# y = [len(t.split()) for t in title]\n",
    "# x = range(0, len(y))\n",
    "# plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7667873",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = train_df['abstract'].tolist()\n",
    "# y = [len(a.split()) for a in abstract]\n",
    "# x = range(0, len(y))\n",
    "# plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec4723fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a916e24263854bb8989eb047517d4e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4042fcdeda47289de3c76eedbac5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256c28a9521416f81afef01e64cbcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/225k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        super(Config, self).__init__()\n",
    "        \n",
    "        self.SEED = 42\n",
    "        self.MODEL_PATH = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract'\n",
    "        self.NUM_LABELS = 1\n",
    "        \n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
    "        self.MAX_LENGTH1 = 72\n",
    "        self.MAX_LENGTH2 = 512\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.N_SPLIT = 5\n",
    "        \n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.FULL_FINETUNING = True\n",
    "        self.LR = 2e-5\n",
    "        self.OPTIMIZER = 'AdamW'\n",
    "        self.CRITERION = 'BCEWithLogitsLoss'\n",
    "        self.SAVE_BEST_ONLY = True\n",
    "        self.N_VALIDATE_DUR_TRAIN = 1\n",
    "        self.EPOCHS = 10\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec6f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_init(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = config.SEED\n",
    "seed_init(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb728a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, df, indices, set_type=None):\n",
    "        super(TransformerDataset, self).__init__()\n",
    "\n",
    "        df = df.iloc[indices]\n",
    "        self.titles, self.abstracts = get_texts(df)\n",
    "        self.set_type = set_type\n",
    "        if self.set_type != 'test':\n",
    "            self.labels = get_labels(df)\n",
    "\n",
    "        self.max_length1 = config.MAX_LENGTH1\n",
    "        self.max_length2 = config.MAX_LENGTH2\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokenized_titles = self.tokenizer.encode_plus(\n",
    "            self.titles[index], \n",
    "            max_length=self.max_length1,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids_titles = tokenized_titles['input_ids'].squeeze()\n",
    "        attention_mask_titles = tokenized_titles['attention_mask'].squeeze()\n",
    "        \n",
    "        tokenized_abstracts = self.tokenizer.encode_plus(\n",
    "            self.abstracts[index], \n",
    "            max_length=self.max_length2,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids_abstracts = tokenized_abstracts['input_ids'].squeeze()\n",
    "        attention_mask_abstracts = tokenized_abstracts['attention_mask'].squeeze()\n",
    "\n",
    "        if self.set_type != 'test':\n",
    "            return {\n",
    "                'titles': {\n",
    "                    'input_ids': input_ids_titles.long(),\n",
    "                    'attention_mask': attention_mask_titles.long(),\n",
    "                },\n",
    "                'abstracts': {\n",
    "                    'input_ids': input_ids_abstracts.long(),\n",
    "                    'attention_mask': attention_mask_abstracts.long(),\n",
    "                },\n",
    "                'labels': torch.Tensor([self.labels[index]]).float(),\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'titles': {\n",
    "                'input_ids': input_ids_titles.long(),\n",
    "                'attention_mask': attention_mask_titles.long(),\n",
    "            },\n",
    "            'abstracts': {\n",
    "                'input_ids': input_ids_abstracts.long(),\n",
    "                'attention_mask': attention_mask_abstracts.long(),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6256359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPubMedBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DualPubMedBert, self).__init__()\n",
    "        self.titles_model = AutoModel.from_pretrained(config.MODEL_PATH)\n",
    "        self.abstracts_model = AutoModel.from_pretrained(config.MODEL_PATH)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.avgpool = nn.AvgPool1d(2, 2)\n",
    "        self.linear = nn.Linear(768, config.NUM_LABELS)\n",
    "    \n",
    "    def forward(self, input_ids_titles, attention_mask_titles=None, input_ids_abstracts=None, attention_mask_abstracts=None):\n",
    "        output = self.titles_model(input_ids=input_ids_titles, attention_mask=attention_mask_titles)\n",
    "        titles_features = output.pooler_output\n",
    "        titles_features = titles_features.unsqueeze(1)\n",
    "        titles_features_pooled = self.avgpool(titles_features)\n",
    "        titles_features_pooled = titles_features_pooled.squeeze(1)\n",
    "        \n",
    "        output = self.abstracts_model(input_ids=input_ids_abstracts, attention_mask=attention_mask_abstracts)\n",
    "        abstracts_features = output.pooler_output\n",
    "        abstracts_features = abstracts_features.unsqueeze(1)\n",
    "        abstracts_features_pooled = self.avgpool(abstracts_features)\n",
    "        abstracts_features_pooled = abstracts_features_pooled.squeeze(1)\n",
    "        \n",
    "        combined_features = torch.cat((titles_features_pooled, abstracts_features_pooled), dim=1)\n",
    "        x = self.dropout(combined_features)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e6d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_dataloader, criterion):\n",
    "    val_loss = 0\n",
    "    true, pred = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        b_input_ids_titles = batch['titles']['input_ids'].to(device)\n",
    "        b_attention_mask_titles = batch['titles']['attention_mask'].to(device)\n",
    "        b_input_ids_abstracts = batch['abstracts']['input_ids'].to(device)\n",
    "        b_attention_mask_abstracts = batch['abstracts']['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids_titles, b_attention_mask_titles, b_input_ids_abstracts, b_attention_mask_abstracts)\n",
    "            loss = criterion(logits, b_labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            logits = torch.sigmoid(logits)\n",
    "            logits = np.where(logits.to('cpu').detach().numpy().copy() < border, 0, 1)\n",
    "            labels = b_labels.to('cpu').detach().numpy().copy()\n",
    "            \n",
    "            pred.extend(logits)\n",
    "            true.extend(labels)\n",
    "        \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print('Val loss:', avg_val_loss)\n",
    "    print('Val accuracy:', accuracy_score(true, pred))\n",
    "    \n",
    "    val_fbeta_score = fbeta_score(true, pred, beta=7.0)\n",
    "    print('Val fbeta score:', val_fbeta_score)\n",
    "    return val_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7998f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch):\n",
    "    nv = config.N_VALIDATE_DUR_TRAIN\n",
    "    temp = len(train_dataloader) // nv\n",
    "    temp = temp - (temp%100)\n",
    "    validate_at_steps = [temp * x for x in range(1, nv+1)]\n",
    "    \n",
    "    train_loss = 0\n",
    "    true, pred = [], []\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), desc='Epoch ' + str(epoch), total=len(train_dataloader)):\n",
    "        \n",
    "        model.train()\n",
    "        b_input_ids_titles = batch['titles']['input_ids'].to(device)\n",
    "        b_attention_mask_titles = batch['titles']['attention_mask'].to(device)\n",
    "        b_input_ids_abstracts = batch['abstracts']['input_ids'].to(device)\n",
    "        b_attention_mask_abstracts = batch['abstracts']['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(b_input_ids_titles, b_attention_mask_titles, b_input_ids_abstracts, b_attention_mask_abstracts)\n",
    "#         print(\"logits=\",logits)\n",
    "        loss = criterion(logits, b_labels)\n",
    "        train_loss += loss.item()\n",
    "#         print(train_loss)\n",
    "        \n",
    "        logits = torch.sigmoid(logits)\n",
    "        logits = np.where(logits.to('cpu').detach().numpy().copy() < border, 0, 1)\n",
    "        labels = b_labels.to('cpu').detach().numpy().copy()\n",
    "            \n",
    "        pred.extend(logits)\n",
    "        true.extend(labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        #if step in validate_at_steps:\n",
    "        #    print(f'-- Step: {step}')\n",
    "        #    _ = val(model, val_dataloader, criterion)\n",
    "            \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    accuracy = accuracy_score(true, pred)\n",
    "    train_fbeta_score = fbeta_score(true, pred, beta=7.0)\n",
    "    print('Training loss:', avg_train_loss)\n",
    "    print('Training accuracy:', accuracy_score)\n",
    "    print('Train fbeta score:', train_fbeta_score)\n",
    "    return avg_train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6866223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dataloader, val_dataloader, writer):\n",
    "    \n",
    "    !jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "    torch.cuda.empty_cache()\n",
    "    model = DualPubMedBert()\n",
    "    model=nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    if config.FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
    "    \n",
    "    num_training_steps = len(train_dataloader) * config.EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    max_val_fbeta_score = float('-inf')\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        avg_train_loss, accuracy = train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n",
    "        val_fbeta_score = val(model, val_dataloader, criterion)\n",
    "        \n",
    "        writer.add_scalar('train_loss', avg_train_loss, epoch+1)\n",
    "        writer.add_scalar('accuracy', accuracy, epoch+1)\n",
    "        writer.add_scalar('val_fbeta_score', val_fbeta_score, epoch+1)\n",
    "        \n",
    "        if config.SAVE_BEST_ONLY:\n",
    "            if val_fbeta_score > max_val_fbeta_score:\n",
    "                max_val_fbeta_score = val_fbeta_score\n",
    "                best_model = copy.deepcopy(model)\n",
    "                \n",
    "    return best_model, max_val_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e85d4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val():\n",
    "    Fold = StratifiedKFold(n_splits=config.N_SPLIT, shuffle=True, random_state=seed)\n",
    "    max_val_fbeta_score = float('-inf')\n",
    "    \n",
    "    for n, (train_indices, val_indices) in enumerate(Fold.split(train_df, train_df['judgement'])):\n",
    "        print(f'========= fold: {n} training =========')\n",
    "        \n",
    "        log_dir = 'logs/fold'+str(n)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        train_data = TransformerDataset(train_df, train_indices)\n",
    "        val_data = TransformerDataset(train_df, val_indices)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n",
    "                \n",
    "        fold_best_model, fold_best_val_fbeta_score = run(train_dataloader, val_dataloader, writer)\n",
    "        \n",
    "        if config.SAVE_BEST_ONLY:\n",
    "            if fold_best_val_fbeta_score > max_val_fbeta_score:\n",
    "                best_model = fold_best_model\n",
    "                max_val_fbeta_score = fold_best_val_fbeta_score\n",
    "                \n",
    "                model_name = 'scibert_dualinput_best_model'\n",
    "                torch.save(best_model.state_dict(), model_name+'.pt')\n",
    "        \n",
    "        writer.close()\n",
    "                \n",
    "    return best_model, max_val_fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "634b88dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = config.DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029102b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= fold: 0 training =========\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707781b3844743cd97e913ae9efa89bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/berry/home/s-kato/.venv/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 4. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1435ac4d2ee41969b45943236794586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/berry/home/s-kato/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.06394516308747358\n",
      "Training accuracy: <function accuracy_score at 0x7fe627437a60>\n",
      "Train fbeta score: 0.7604628603104212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303d077ba27645a592ff2dcf6e6ebc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/berry/home/s-kato/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.04556104031884495\n",
      "Val accuracy: 0.8905875851906428\n",
      "Val fbeta score: 0.8928571428571428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866c11a5c8314e24a499b3a74b8a0b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/berry/home/s-kato/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model, best_val_fbeta_score = cross_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_val_fbeta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7373cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(test_df)\n",
    "test_indices = list(range(dataset_size))\n",
    "test_data = TransformerDataset(test_df, test_indices, set_type='test')\n",
    "test_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)\n",
    "\n",
    "\n",
    "def predict(model):\n",
    "    val_loss = 0\n",
    "    test_pred = []\n",
    "    model.eval()\n",
    "    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "#         batch = batch[0]\n",
    "        b_input_ids_titles = batch['titles']['input_ids'].to(device)\n",
    "        b_attention_mask_titles = batch['titles']['attention_mask'].to(device)\n",
    "        b_input_ids_abstracts = batch['abstracts']['input_ids'].to(device)\n",
    "        b_attention_mask_abstracts = batch['abstracts']['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids_titles, b_attention_mask_titles, b_input_ids_abstracts, b_attention_mask_abstracts)\n",
    "            logits = torch.sigmoid(logits)\n",
    "            logits = np.where(logits.to('cpu').detach().numpy().copy() < border, 0, 1)\n",
    "            test_pred.extend(logits)\n",
    "    \n",
    "    test_pred = np.array(test_pred)\n",
    "    return test_pred\n",
    "\n",
    "test_pred = predict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit():\n",
    "    sample_submission = pd.read_csv('data/sample_submit.csv', names=('id', 'judgement'))\n",
    "    ids = sample_submission['id'].values.reshape(-1,1)\n",
    "    \n",
    "    merged = np.concatenate((ids, test_pred), axis=1)\n",
    "    submission = pd.DataFrame(merged, columns=sample_submission.columns).astype(int)\n",
    "    return submission\n",
    "\n",
    "submission = submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('output/outputs.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8101f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation accuracyをtensorboardで管理\n",
    "#k_foldのscoreの平均値を出す\n",
    "#f1からbetaスコアに変える"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aac5378b2f659b7e03042f36883bbb0c3f20ccc6d1897585acac59e3bb2b9166"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
